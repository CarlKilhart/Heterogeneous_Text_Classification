import torch
import torch.nn as nn
import dgl
import dgl.nn as dnn

from utils.helper import activation_helper
from utils.aggregator import SelfAttention

DIM_EMBEDDING = 100             # Dim of embedding for each node in graph, generated by GCN
DIM_LIGHTGCN_EMBEDDING = 300    # Dim of embedding for each node in graph, used by LightGCN

class GCN(nn.Module):
    def __init__(self, dim_in:int, dim_hidden:int, dim_out:int, activation:str='relu'):
        super(GCN, self).__init__()
        activation = activation_helper(activation)
        
        self.layer1 = dnn.GraphConv(dim_in, dim_hidden, activation=activation())
        self.dropout = nn.Dropout(0.5)
        self.layer2 = dnn.GraphConv(dim_hidden, dim_out)
        
    def forward(self, g:dgl.DGLGraph, features:torch.Tensor, edge_weight:torch.Tensor=None):
        x = self.layer1(g, features, edge_weight=edge_weight)
        x = self.dropout(x)
        out = self.layer2(g, x, edge_weight=edge_weight)
        
        return out

class HGCN(nn.Module):
    def __init__(self, num_graph:int, target_index:list, dim_in:int, dim_hidden:int, dim_out:int, activation:str='relu'):
        super(HGCN, self).__init__()
        self.target_index = target_index
        self.gcn_list = [GCN(dim_in, dim_hidden, dim_out, activation) for _ in range(num_graph)]
        self.gcn_list = nn.ModuleList(self.gcn_list)
        self.aggregator = SelfAttention(dim_out, dim_out)
    
    def forward(self, graph_list:list, feature_list:list):
        out_list = []
        for gcn, graph, feature in zip(self.gcn_list, graph_list, feature_list):
            out = gcn(graph, feature)
            out_list.append(out[self.target_index])
        out = torch.concatenate([t.unsqueeze(1) for t in out_list], dim=1)
        out = self.aggregator(out)
        return out